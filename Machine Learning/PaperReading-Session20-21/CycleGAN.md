# Paper Reading Session 04
**Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks** by *Zhu, Jun Yan Park, Taesung Isola, Phillip Efros, Alexei A.*.

## Resources
Paper link: [CycleGAN](https://arxiv.org/abs/1703.10593)

### Introduction
- Introduces an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples.
-	In absence of paired examples, authors try to exploit supervision at the level of sets(input set X and output set Y). In theory, one can produce an output distribution Y' which distributes identically to Y but such a translation dosn't guarantee to pair an individual x to the desired y. Moreover, in practice, standard adversial objectives are difficult to optimize (e.g mode collapse)
-	To overcome these problems, authors introduce the concept of "Cycle Consistency" which states that if we have a translator G : X → Y and another translator F : Y → X, then G and F should be inverses of each other. So, the model, in addition to the standard adversial loss, also has a cycle consistency loss.
- The model builds on the “pix2pix” frame-work which uses a conditional GAN to learn a mapping from input to output images.

### Formulation
![Diagram](https://media.geeksforgeeks.org/wp-content/uploads/20200529210740/cycleconsistencyandlosses.PNG)


- The Model includes two mappings G : X → Y and F : Y → X. In addition, two adversarial discriminators DX and DY, where DX aims to distinguish between images {x} and translated images {F(y)}; in the same way, DY aims to discriminate between {y} and {G(x)}
- Adversial loss:  LGAN(G,D,X,Y) = Ey∼pdata(y)[logD(y)] + Ex∼pdata(x)[log(1 −D(G(x))]
For both individual mappings G and F, it has separate adversial loss. 
- Cycle consistency loss: Lcyc(G,F) = Ex∼pdata(x)[||F(G(x)) − x1||] + Ey∼pdata(y)[||G(F(y)) − y1||].
It encourages F(G(x)) ≈ x and G(F(y)) ≈ y.
- Full objective: L(G,F,DX,DY) = LGAN(G,DY,X,Y) + LGAN(F,DX,Y,X) + λLcyc(G,F)


### Implementation
#### Network Architecture: 
- Generator: This network contains three convolutions, several residual blocks, two fractionally-strided convolutions with stride 1/2, and one convolution that maps features to RGB. It also uses Instance Normalization.
- Discriminator: 70 × 70 PatchGANs which aim to classify whether 70 × 70 overlapping image patches are real or fake. Such a patch-level discriminator architecture has fewer parameters than a full-image discriminator and can work on arbitrarily sized images in a fully convolutional fashion.
#### Training: 
- For LGAN, the negative log likelihood objective is replaced by a least-squares loss. This loss is more stable during training and generates higher quality results. 
- To reduce model oscillation, the discriminators are updated using a history of generated images rather than the ones produced by the latest generators. An image buffer is kept that stores the 50 previously created images.

### Evaluation Metrics
- AMT: Participants were shown a sequence of pairs of images, one a real photo or map and one fake (generated by our algorithm), and asked to click on the image they thought was real.
- FCN: The FCN predicts a label map for a generated photo. This label map can then be compared against the input ground truth labels using standard semantic segmentation metric. The intuition is that if we generate a photo from a label map of “car on the road”, then we have succeeded if the FCN applied to the generated photo detects “car on the road”.
- Semantic Segmentation Metric: per-pixel accuracy, per-class accuracy, and mean class Intersection-Over-Union.

### Result
- The model performs better than the prior methods on a range of tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc.
- Analysis of the Loss function: Removing the GAN loss substantially degrades results, as does removing the cycle-consistency loss. It therefore concludes that both terms are critical. Using CycleGAN only in one direction often incurs training instability and causes mode collapse producing identical label maps regardless of the input photo, especially for the direction of the mapping that was removed.






## Credits
- Meeting conducted by: [Udbhav Bamba](https://github.com/ubamba98)
- Paper explained by: [Prateek Bedre](https://github.com/pratikb2805)
- Report compiled by: [Taresh Rajput](https://github.com/taresh18)
- Dated 31/08/20
