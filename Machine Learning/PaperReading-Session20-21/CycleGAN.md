# Paper Reading Session 04
**Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks** by *Zhu, Jun Yan Park, Taesung Isola, Phillip Efros, Alexei A.*.

## Resources
Paper link: [CycleGAN](https://arxiv.org/abs/1703.10593)

### Introduction
- Introduces an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples.
-	Goal is to learn a mapping G : X → Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, couples it with an inverse mapping F : Y → X and introduces a cycle consistency loss to enforce F(G(X)) ≈ X (and vice versa).
-	

### Formulation
![Diagram](https://media.geeksforgeeks.org/wp-content/uploads/20200529210740/cycleconsistencyandlosses.PNG)


- Our goal is to learn mapping functions between two domains X and Y given training samples {xi}N i=1 where xi ∈ X and {yj}M j=1 where yj ∈ Y1. We denote the data
distribution as x ∼ pdata(x) and y ∼ pdata(y).our model includes two mappings G : X → Y and F : Y → X. In addition, we in- troduce two adversarial discriminators DX and DY, where DX aims to distinguish between images {x} and translated images {F(y)}; in the same way, DY aims to discriminate between {y} and {G(x)}
- Adversial loss: LGAN(G,D,X, Y) = Ey∼pdata(y)[logD(y)] + Ex∼pdata(x)[log(1 −DY(G(x))]
- Cycle consistency loss: Lcyc(G,F) = Ex∼pdata(x)[||F(G(x)) − x1||] + Ey∼pdata(y)[||G(F(y)) − y1||].
- Full objective: L(G,F,DX,DY) =LGAN(G,DY,X,Y) + LGAN(F,DX,Y,X) + λLcyc(G,F)

- Removing the GAN loss substantially degrades results, as does removing the cycle-consistency loss. We therefore conclude that both terms are critical to our results. Using CycleGAN only in one direction often incurs training instability and causes mode collapse producing identical label maps regardless of the input photo, especially for the direction of the mapping that was removed


### Implementation
#### Network Architecture: 
- Generator: Uses 6 residual blocks for 128 × 128 training images, and 9 residual blocks for 256 × 256 or higher-resolution training images.
- Discriminator: 70 × 70 PatchGANs which aim to classify whether 70 × 70 overlapping image patches are real or fake
#### Training: 
- For LGAN, we replace the negative log likelihood objective by a least-squares loss. This loss is more stable during training and generates higher quality results. 
- To reduce model oscillation, we update the discriminators using a history of generated images rather than the ones produced by the latest generators. We keep an image buffer that stores the 50 previously created images.

### Evaluation
- AMT: Participants were shown a sequence of pairs of images, one a real photo or map and one fake (generated by our algorithm), and asked to click on the image they thought was real.
- FCN: The FCN predicts a label map for a generated photo. This label map can then be compared against the input ground truth labels using standard semantic segmentation metric. The intuition is that if we generate a photo from a label map of “car on the road”, then we have succeeded if the FCN applied to the generated photo detects “car on the road”.
- Semantic Segmentation Metric: per-pixel accuracy, per-class accuracy, and mean class Intersection-Over-Union

### Conclusion





## Credits
- Meeting conducted by: [Udbhav Bamba](https://github.com/ubamba98)
- Paper explained by: [Prateek Bedre](https://github.com/pratikb2805)
- Report compiled by: [Taresh Rajput](https://github.com/taresh18)
- Dated 31/08/20
