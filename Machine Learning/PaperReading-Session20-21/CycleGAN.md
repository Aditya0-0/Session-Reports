# Paper Reading Session 04
**Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks** by *Zhu, Jun Yan Park, Taesung Isola, Phillip Efros, Alexei A.*.

## Resources
Paper link: [CycleGAN](https://arxiv.org/abs/1703.10593)

### Introduction
- Introduces an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples.
-	In absence of paired examples, we try to exploit supervision at the level of sets(input set X and output set Y). In theory, we can produce an output distribution Y' which distributes identically to Y but such a translation dosn't guarantee to pair an individual x to the desired y. Moreover, in practice, standard adversial objectives are difficult to optimize (e.g mode collapse)
-	To overcome these problems, the author introduce the concept of "Cycle Consistency" which states that if we have a translator G : X → Y and another translator F : Y → X, then G and F should be inverses of each other. So, the model, in addition to the standard adversial loss, also has a cycle consistency loss.
- The model builds on the “pix2pix” frame-work which uses a conditional GAN to learn a mapping from input to output images.

### Formulation
![Diagram](https://media.geeksforgeeks.org/wp-content/uploads/20200529210740/cycleconsistencyandlosses.PNG)


- The Model includes two mappings G : X → Y and F : Y → X. In addition, two adversarial discriminators DX and DY, where DX aims to distinguish between images {x} and translated images {F(y)}; in the same way, DY aims to discriminate between {y} and {G(x)}
- Adversial loss:  LGAN(G,D,X, Y) = Ey∼pdata(y)[logD(y)] + Ex∼pdata(x)[log(1 −DY(G(x))]
G tries to generate images G(x) that look similar to images from domain Y, while DY aims to distinguish be- tween translated samples G(x) and real samples y
- Cycle consistency loss: Lcyc(G,F) = Ex∼pdata(x)[||F(G(x)) − x1||] + Ey∼pdata(y)[||G(F(y)) − y1||].
It encourages F(G(x)) ≈ x and G(F(y)) ≈ y.
- Full objective: L(G,F,DX,DY) =LGAN(G,DY,X,Y) + LGAN(F,DX,Y,X) + λLcyc(G,F)

- Removing the GAN loss substantially degrades results, as does removing the cycle-consistency loss. We therefore conclude that both terms are critical to our results. Using CycleGAN only in one direction often incurs training instability and causes mode collapse producing identical label maps regardless of the input photo, especially for the direction of the mapping that was removed


### Implementation
#### Network Architecture: 
- Generator: Uses 6 residual blocks for 128 × 128 training images, and 9 residual blocks for 256 × 256 or higher-resolution training images.
- Discriminator: 70 × 70 PatchGANs which aim to classify whether 70 × 70 overlapping image patches are real or fake
#### Training: 
- For LGAN, we replace the negative log likelihood objective by a least-squares loss. This loss is more stable during training and generates higher quality results. 
- To reduce model oscillation, we update the discriminators using a history of generated images rather than the ones produced by the latest generators. We keep an image buffer that stores the 50 previously created images.

### Evaluation
- AMT: Participants were shown a sequence of pairs of images, one a real photo or map and one fake (generated by our algorithm), and asked to click on the image they thought was real.
- FCN: The FCN predicts a label map for a generated photo. This label map can then be compared against the input ground truth labels using standard semantic segmentation metric. The intuition is that if we generate a photo from a label map of “car on the road”, then we have succeeded if the FCN applied to the generated photo detects “car on the road”.
- Semantic Segmentation Metric: per-pixel accuracy, per-class accuracy, and mean class Intersection-Over-Union

### Conclusion





## Credits
- Meeting conducted by: [Udbhav Bamba](https://github.com/ubamba98)
- Paper explained by: [Prateek Bedre](https://github.com/pratikb2805)
- Report compiled by: [Taresh Rajput](https://github.com/taresh18)
- Dated 31/08/20
